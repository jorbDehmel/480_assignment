\documentclass[12pt]{amsart}

\usepackage[margin=0.75in]{geometry}

\title{Comparison of Sorting Algorithms}
\author{Jordan Dehmel, Kate Eckhart, Logan Humbert, Darrin Miller}
\date{October 8th, 2023}

\begin{document}

\maketitle

\section{Heapsort}
Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure to efficiently sort an array in ascending or descending order. It works by building a max-heap (for ascending order) or a min-heap (for descending order) from the input array and repeatedly extracting the root element to place it in its correct position.
\newline

The algorithm can be divided into the following steps:

\begin{enumerate}
    \item \textbf{Build Heap:} Convert the input array into a max-heap (or min-heap) structure.
    \item \textbf{Heapify:} Recursively remove the root element (the maximum or minimum, depending on the order) and replace it with the last element of the heap. After each removal, heapify the remaining heap to maintain the heap property.
    \item \textbf{Sorted Array:} The sorted array is constructed by repeatedly extracting the root element from the heap and placing it at the end of the array. The remaining elements in the heap are then heapified.
\end{enumerate}

Heap Sort has a time complexity of $O(n \log n)$. This is because the heapify operation takes $O(\log n)$ time and is performed $n$ times.\newline
The space complexity is $O(\log n)$ because of the recursive nature of the heapify operation.
An iterative implementation of heapify can be used to reduce the space complexity to $O(1)$.
\newline

Heap Sort's efficiency and stability make it a useful choice in various applications where a stable sorting algorithm with a predictable time complexity is required.

\section{Mergesort}
    Lorem ipsum

\section{Priority Queue Using an Ordered Array}
    Lorem ipsum

\section{LSD Radix String Sort}

    \subsection{Algorithmic Outline}

    LSD Radix string sort is an optimization of key-indexed
    counting. Key-indexed counting, also known as counting sort,
    requires auxiliary space with size proportional to the
    number of possible keys (for 8-bit chars it would require
    space proportional to $255$, for 128-bit integers it would
    require space proportional to $2^{128}$, etc.). This
    becomes a major problem when sorting structures of
    unspecified or very large size- for instance, strings. A
    string could be empty, or it could contain the collected
    works of Shakespeare, for instance. This means that counting
    sort would have to allocate near-infinite memory for the
    counting of strings.
    
    Radix sort solves this problem by iterating over discrete
    ordered chunks (radii) within the data, sorting one after
    another. For instance, it may sort the 1's place of an
    integer, then the 10's place, and so on. This leads to a
    space requirement of only $10$ in this instance. In a
    string, this amounts to iteratively sorting the strings by
    their character in a given position, reducing the space
    requirement from near-infinite to only $255$ (the number of
    possible chars).

    LSD radix string sort is an instance of radix string sort
    wherein strings are first sorted by their least-significant
    digit (their rightmost char), and the sorting process
    iterates leftwards.

    \subsection{Theoretical Time-Space Complexity Analysis}

    First, we will analyze the complexities of counting sort.
    Counting sort iterates a constant number of times over the
    inputs. Thus, it is colloquially said to have time
    complexity $O(n)$. However, a more thorough analysis reveals
    that it must also iterate over its internal counting array,
    which has size proportional to the number of possible items
    in a given place in the input. If we define this number as
    $m_t$, then we must say counting sort has a time complexity of
    $O(n + m_t)$. The space complexity is $O(n + m_t)$, since
    counting sort requires both a counting array and out output
    array.

    Secondly, we will analyze the complexities of LSD radix
    sort. If we say $m_t$ is the total number of possible keys
    in a given item of the array, $s$ is the number of ordered
    discrete subsections within the item, and $m_i$ is the
    number of possible values in a given subsection, we can say
    that

    \[
    \begin{aligned}
        m_t = m_i^s            \\
        \sqrt[s]{m_t} = m_i    \\
    \end{aligned}
    \]

    This is not strictly necessary, but it is nice to have the
    complexities for radix and counting sort in terms of the
    same units.

    Radix sort calls counting sort for each of the items
    within. Thus, we can say that radix sort runs with time complexity
    $O(s \cdot n + s \cdot \sqrt[s]{m_t}) = O(s \cdot n + s \cdot m_i)$.
    Similarly, we say that radix sort runs with space complexity
    $O(n + \sqrt[s]{m_t}) = O(n + m_i)$. This makes radix sort
    much better suited to large items like strings.

    \subsection{Observational Time-Space Complexity}

    LSD Radix sort primarily sorts strings based on length,
    since it treats extra MSD characters as zeros. For strings
    of equal length, it sorts alphabetically from the least to
    most significant digit.

    LSD radix string sort runs in time complexity $O(mn)$, where
    $m$ is the max string length and $n$ is the number of
    strings.

    The following data is from multiple runs of the sort using
    various input data, mostly variations on "lorem ipsum".

    \newpage

\begin{verbatim}
Input size:	113
Number array accesses:	41278
Execution milliseconds:	0.877957
Total bytes used:	7260
Array accesses / input size:    365.29
Bytes / input size: 64.25

Input size:	444
Number array accesses:	130979
Execution milliseconds:	2.3243
Total bytes used:	22817
Array accesses / input size:    294.99
Bytes / input size: 51.38

Input size:	891
Number array accesses:	252116
Execution milliseconds:	5.5201
Total bytes used:	43826
Array accesses / input size:    282.96
Bytes / input size: 49.19

Input size:	2414
Number array accesses:	664849
Execution milliseconds:	5.34206
Total bytes used:	115407
Array accesses / input size:    275.41
Bytes / input size: 47.81

Input size:	25974
Number array accesses:	7049609
Execution milliseconds:	215.338
Total bytes used:	1222727
Array accesses / input size:    271.41
Bytes / input size: 47.07

Input size:	135215
Number array accesses:	36653920
Execution milliseconds:	244.25
Total bytes used:	6357054
Array accesses / input size:    271.08
Bytes / input size: 47.01

Input size:	1336135
Number array accesses:	362103240
Execution milliseconds:	1339.62
Total bytes used:	62800294
Array accesses / input size:    271.01
Bytes / input size: 47.00

Input size:	2704186
Number array accesses:	732845061
Execution milliseconds:	2677.49
Total bytes used:	127098691
Array accesses / input size:    271.00
Bytes / input size: 47.00

\end{verbatim}

    Observationally, the time requirement of radix string sort
    is equal to $271n$ for input size $n$ as $n \to \infty$. The
    space complexity is similarly equal to $47n$. This leaves
    us with observational time and space complexities of $O(n)$.

    It must be noted that our sample data is fairly repetitive,
    having mostly been generated via a "lorem ipsum" generator,
    and our substring lengths are limited to 15. Larger
    substrings will lead to larger coefficients on $n$, but will
    not change the time complexity, as long as they are not
    proportional to the size of the input array.

    The reason substring lengths were limited to 15 is because,
    were they not limited as such, each item would have a
    different length from all the other items. This would cause
    the sort to sort exclusively by length, and not at all
    lexicographically. By limiting substring lengths to 15, we
    still have a number of strings which vary in length, while
    still having a good amount of lexicographical sorting.

\end{document}

\documentclass[12pt]{amsart}

\usepackage[margin=0.75in]{geometry}

\title{Comparison of Sorting Algorithms}
\author{Jordan Dehmel, Kate Eckhart, Logan Humbert, Darrin Miller}
\date{October 8th, 2023}

\begin{document}

\maketitle

\section{Heapsort}
Heap Sort is a comparison-based sorting algorithm that uses a binary heap data structure to efficiently sort an array in ascending or descending order. It works by building a max-heap (for ascending order) or a min-heap (for descending order) from the input array and repeatedly extracting the root element to place it in its correct position.
\newline

The algorithm can be divided into the following steps:

\begin{enumerate}
    \item \textbf{Build Heap:} Convert the input array into a max-heap (or min-heap) structure.
    \item \textbf{Heapify:} Recursively remove the root element (the maximum or minimum, depending on the order) and replace it with the last element of the heap. After each removal, heapify the remaining heap to maintain the heap property.
    \item \textbf{Sorted Array:} The sorted array is constructed by repeatedly extracting the root element from the heap and placing it at the end of the array. The remaining elements in the heap are then heapified.
\end{enumerate}

Heap Sort has a time complexity of $O(n)$ in its best case, and $O(2 n \log n)$ in its average and worst cases. \newline
The space complexity is less than $O(2 n \log n)$ because of the recursive nature of the heapify operation.
An iterative implementation of heapify can be used to reduce the space complexity to $O(1)$.
\newline

These values are Theoretical. If we look at the results of our tests, we have approximatively 16825 comparisons and 9056 swaps, to sort 1000 random numbers.
2*1000 ln(1000) = 13815.5, which is less than 16825 + 9056 = 25881.

Concerning the time complexity, we have approximatively 476669 nano seconds to sort 1000 random numbers. 2*1000 ln(1000) = 13815.5, which is <= 476669.

\section{Mergesort}
Merge sort is a divide-and-conquer, comparison-based sorting algorithm that is widely 
used for sorting large datasets. Merge sort is a stable algorithm, maintaining relative 
position of equal elements. It is not an in-place algorithm and does require temporary 
arrays. It is one of the most consistent sorting algorithms with regard to performance, 
having a time complexity of $O(n \log n)$ in the worst, best, and average cases. It works 
by recursively dividing the list into sublists and then merging these sublists into a 
sorted list until the entire list has been sorted and merged.
\newline
\newline
The algorithm can be divided into the following steps:
\begin{enumerate}
    \item \textbf{Divide List:}Start with an unsorted list. Divide it into two equal 
                        (or nearly equal) halves. Continue this division recursively 
                        until each sublist contains only one element. This recursive 
                        division is fundamental to the "divide and conquer" approach 
                        of merge sort. Base Case: The division continues until each 
                        sublist contains only one element. This is the base case of 
                        the recursion.
    \item \textbf{Merge Sublists:}After achieving the base case, start merging the 
                        sublists back together. The merging process involves comparing 
                        and combining elements from the left and right sublists, one 
                        element at a time. 
    \item \textbf{Recursive Merge:}The "merge" step is performed recursively on the 
                        sublists. First, the sublists containing one element are merged 
                        into sublists containing two elements, then those are merged 
                        into sublists containing four elements, and so on. This process 
                        continues until the entire original list is sorted.
\end{enumerate}

\begin{table}[ht]
    \centering
    \caption{Characteristics of Merge Sort}
    \begin{tabular}{|c|c|}
    \hline
    Characteristic & Description \\
    \hline
    Stability & Stable (Preserves relative order of equal elements) \\
    In-Place & Not In-Place (Requires additional memory for sublists) \\
    Time Complexity & $O(n \log n)$ \\
    Space Complexity & $O(n)$ \\
    Parallelizability & Efficiently parallelizable\\
    Data Type & Wide range data types, structures and custom comparisons\\
    \hline
    \end{tabular}
    \end{table}

\begin{table}[ht]
    \centering
    \caption{Results from testing}
    \begin{tabular}{|c|c|}
    \hline
    Variable & Value \\
    \hline
    Input size & 1000\\
    Input bytes & 8000\\
    Number comparisons & 8708\\
    Number swaps(assignments) & 9987\\
    Execution nanoseconds & 132783\\
    Execution milliseconds & 0.132783\\
    Max concurrent bytes used & 8008\\
    Total bytes used & 79896\\
    \hline
    \end{tabular}
    \end{table}

Final notes: The comparisons are less than assignments because if one of the subarrays being merged is empty the rest of the remaining array can be added without any comparison. The space complexity is shown to be O(n+1) equivalent to O(n)
As mentioned above, merge sort is often used on large datasets and is valued for its consistent performance. However, the space complexity should be considered in applications with limited memory resources.





\section{Priority Queue Using an Ordered Array}
    \subsection{Algorithm}
    \begin{enumerate}
        \item
            A sanity check is done for empty queue's
         \item
             The best known greatest item starts as the item with index 0.
         \item
             Starting with the item with index 1: see if it's greater than the best known greatest item
         \item
             If so, that item is now the best known greatest item.
         \item
             Repeat 3,4 on the next element until the entire array has been scanned.
         \item
             Swap the best known greatest item with the last item. (Can be a trivial swap; best known greatest item may be the last)
         \item
             Shrink the size of the priority queue by one.
         \item
             Return the item just removed.
    \end{enumerate}

    \subsection{Performance}
    
    The performance assuming you call deleteMax until the Priority Queue is empty.
    
    The good news, the amount of swaps is $O(n)$ (exactly n). The bad news, the amount of compares is $O(n^2)
$. 
    Swaps are easy to figure out as deleteMax must be called once per element and that function swaps only after the loop. \\

    The amount of compares can be shown to be $(n - 1) + (n - 2) + (n - 3) + ... + 0$ because the first deleteMax assumes one the elements is the max. Then for every other element in the list a single compare is needed to decide if the current element or the best known greatest item is the new best known greatest item.
    For the second deleteMax an analogous process happens except the array is one less. And so on.
    This sequence can be collapsed into $\frac{n(n - 1)}{2}$ due to it being easy rearrangeable into $1 + 2 + 3 + 4 + .... + n - 1$, the triangle number sequence.
    The collapsed version is easily seen as $O(n^2)$.
    Since the analysis never needs to use the exact value of the compare or other properties of data other than length, it's best, average and worst are the same.
    The queue is only 13 times slower than heapsort while having 30 times the compares and 9 times less swaps.
    I assume this has to do with the fact that the compares are done in sequential order (cpu cache magic: one of the elements is almost guarantied to be cached as it was used last iteration, the other element was adjacent to one read giving a high likelihood of it also being cached) and having the minimal amount of swaps.
    Compared to heap sort which access its backing array in a semi-random order.
    The ram usage should be about the same but the ram tracking code for heap sort is \textbf{way} overestimating because both algorithms only need a small constant amount of memory plus the backing array.
\begin{verbatim}
Heap Sort:
Stable: false
In-place:       true
Input size:     1000
Number comparisons:     16832
Number swaps:   9065
Execution nanoseconds:  1079755
Execution milliseconds: 1.07976
Max concurrent bytes used:      145040
Total bytes used:       145040

Priority Queue:
Stable: false
In-place:       true
Input size:     1000
Number comparisons:     499500
Number swaps:   1000
Execution nanoseconds:  12701510
Execution milliseconds: 12.7015
Max concurrent bytes used:      4136
Total bytes used:       4136
\end{verbatim}

\section{LSD Radix String Sort}

    \subsection{Algorithmic Outline}

    LSD Radix string sort is an optimization of key-indexed
    counting. Key-indexed counting, also known as counting sort,
    requires auxiliary space with size proportional to the
    number of possible keys (for 8-bit chars it would require
    space proportional to $255$, for 128-bit integers it would
    require space proportional to $2^{128}$, etc.). This
    becomes a major problem when sorting structures of
    unspecified or very large size- for instance, strings. A
    string could be empty, or it could contain the collected
    works of Shakespeare, for instance. This means that counting
    sort would have to allocate near-infinite memory for the
    counting of strings.

    Radix sort solves this problem by iterating over discrete
    ordered chunks (radii) within the data, sorting one after
    another. For instance, it may sort the 1's place of an
    integer, then the 10's place, and so on. This leads to a
    space requirement of only $10$ in this instance. In a
    string, this amounts to iteratively sorting the strings by
    their character in a given position, reducing the space
    requirement from near-infinite to only $255$ (the number of
    possible chars).

    LSD radix string sort is an instance of radix string sort
    wherein strings are first sorted by their least-significant
    digit (their rightmost char), and the sorting process
    iterates leftwards.

    \subsection{Theoretical Time-Space Complexity Analysis}

    First, we will analyze the complexities of counting sort.
    Counting sort iterates a constant number of times over the
    inputs. Thus, it is colloquially said to have time
    complexity $O(n)$. However, a more thorough analysis reveals
    that it must also iterate over its internal counting array,
    which has size proportional to the number of possible items
    in a given place in the input. If we define this number as
    $m_t$, then we must say counting sort has a time complexity of
    $O(n + m_t)$. The space complexity is $O(n + m_t)$, since
    counting sort requires both a counting array and out output
    array.

    Secondly, we will analyze the complexities of LSD radix
    sort. If we say $m_t$ is the total number of possible keys
    in a given item of the array, $s$ is the number of ordered
    discrete subsections within the item, and $m_i$ is the
    number of possible values in a given subsection, we can say
    that

    \[
    \begin{aligned}
        m_t = m_i^s            \\
        \sqrt[s]{m_t} = m_i    \\
    \end{aligned}
    \]

    This is not strictly necessary, but it is nice to have the
    complexities for radix and counting sort in terms of the
    same units.

    Radix sort calls counting sort for each of the items
    within. Thus, we can say that radix sort runs with time complexity
    $O(s \cdot n + s \cdot \sqrt[s]{m_t}) = O(s \cdot n + s \cdot m_i)$.
    Similarly, we say that radix sort runs with space complexity
    $O(n + \sqrt[s]{m_t}) = O(n + m_i)$. This makes radix sort
    much better suited to large items like strings.

    \subsection{Observational Time-Space Complexity}

    LSD Radix sort primarily sorts strings based on length,
    since it treats extra MSD characters as zeros. For strings
    of equal length, it sorts alphabetically from the least to
    most significant digit.

    LSD radix string sort runs in time complexity $O(mn)$, where
    $m$ is the max string length and $n$ is the number of
    strings.

    The following data is from multiple runs of the sort using
    various input data, mostly variations on "lorem ipsum".

    \newpage

\begin{verbatim}
Input size:	113
Number array accesses:	41278
Execution milliseconds:	0.877957
Total bytes used:	7260
Array accesses / input size:    365.29
Bytes / input size: 64.25

Input size:	444
Number array accesses:	130979
Execution milliseconds:	2.3243
Total bytes used:	22817
Array accesses / input size:    294.99
Bytes / input size: 51.38

Input size:	891
Number array accesses:	252116
Execution milliseconds:	5.5201
Total bytes used:	43826
Array accesses / input size:    282.96
Bytes / input size: 49.19

Input size:	2414
Number array accesses:	664849
Execution milliseconds:	5.34206
Total bytes used:	115407
Array accesses / input size:    275.41
Bytes / input size: 47.81

Input size:	25974
Number array accesses:	7049609
Execution milliseconds:	215.338
Total bytes used:	1222727
Array accesses / input size:    271.41
Bytes / input size: 47.07

Input size:	135215
Number array accesses:	36653920
Execution milliseconds:	244.25
Total bytes used:	6357054
Array accesses / input size:    271.08
Bytes / input size: 47.01

Input size:	1336135
Number array accesses:	362103240
Execution milliseconds:	1339.62
Total bytes used:	62800294
Array accesses / input size:    271.01
Bytes / input size: 47.00

Input size:	2704186
Number array accesses:	732845061
Execution milliseconds:	2677.49
Total bytes used:	127098691
Array accesses / input size:    271.00
Bytes / input size: 47.00

\end{verbatim}

    Observationally, the time requirement of radix string sort
    is equal to $271n$ for input size $n$ as $n \to \infty$. The
    space complexity is similarly equal to $47n$. This leaves
    us with observational time and space complexities of $O(n)$.

    It must be noted that our sample data is fairly repetitive,
    having mostly been generated via a "lorem ipsum" generator,
    and our substring lengths are limited to 15. Larger
    substrings will lead to larger coefficients on $n$, but will
    not change the time complexity, as long as they are not
    proportional to the size of the input array.

    The reason substring lengths were limited to 15 is because,
    were they not limited as such, each item would have a
    different length from all the other items. This would cause
    the sort to sort exclusively by length, and not at all
    lexicographically. By limiting substring lengths to 15, we
    still have a number of strings which vary in length, while
    still having a good amount of lexicographical sorting.

\end{document}
